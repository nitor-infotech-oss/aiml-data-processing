{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "    It is a subset of artificial intelligence which focuses on using the statistical methods, to build intelligent computer systems in order to learn from the database available to it.\n",
    "    The intelligent computer systems built on machine learning have the capability to learn from the past experience without having to explicitly program. Primary aim is to allow computer systems learn automatically without human interventions.\n",
    "    ML is being used in multiple fields like medical diagnosis, image processing, prediction, classification etc.\n",
    "    \n",
    "## Types of ML algorithms\n",
    "Mainly divided in four categories:\n",
    "### Supervised Learning\n",
    "* In supervised learning all materials are \"labeled\" to tell the machine the corresponding value to make it predict the correct value. This method is mostly manual classification, which is the easiest for a computer and the hardest for humans.\n",
    "* This method is like telling the machine, standard answer. When the machine is officially tested, the machine will reply according to the standard answer.\n",
    "* For example, if you train a machine to distinguish between elephants and giraffes, you can provide 100 photos of elephants and giraffes. The machine detects the characteristics of elephants and giraffes according to the \"labeled\" photographs and identifies elephants and giraffes according to their characteristics.\n",
    "\n",
    "### Un-supervised Learning\n",
    "* In un-supervised learning no material is labeled and machine classifies the materials by detecting the characteristics of the data.\n",
    "* Manual classification is not done in this method, which is simplest for humans, but hardest for the computers and caused more errors.\n",
    "* If you asked machines to identify elephants and giraffes, the machine must decide which of the 100 photos provided are elephants and which are giraffes and do the classification at the same time. Machine must classify animals according to the characteristics and the results identified by machines can be wrong as well.\n",
    "\n",
    "### Semi-supervised Learning\n",
    "* In semi-supervised learning small amount of data are labeled. Computers only need to find features through labeled data and then classify other data accordingly. This method can make predictions more accurately and is the most commonly used method.\n",
    "*  If there are 100 photos, 10 of them which are elephants and which are giraffes are labeled. Through the characteristics of these 10 photos, the machine identifies and classifies the remaining photos.\n",
    "* The results are more accurate than unsupervised learning.\n",
    "\n",
    "### Reinforcement Learning\n",
    "* Reinforcement Learning is a feedback-based Machine learning technique in which an agent learns to behave in an environment by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback or rewards, and for each bad action, the agent gets negative feedback or penalty.\n",
    "* The artificial intelligence gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward.\n",
    "* The agent learns automatically using feedbacks without any labeled data. Since there is no labeled data, so the agent has to learn by its experience only. The primary goal of an agent in reinforcement learning is to improve the performance by getting the maximum positive rewards. The total number of rewards to reach the final goal will help an agent to improve its specific action.\n",
    "* For example chess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is a process of preparing the raw data and making it suitable for a machine learning model. It is the first and crucial step while creating a machine learning model.\n",
    "\n",
    "Below are the steps of preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n",
      "\n",
      "\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "\n",
      "\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = pd.read_csv(r\"C:\\Users\\kanki\\OneDrive\\Documents\\python\\Data1.csv\")\n",
    "print(dataset)\n",
    "print('\\n')\n",
    "x = dataset.iloc[: , :-1].values  #if we don't use .values then column names and index names will be stored in assigned variable\n",
    "y = dataset.iloc[: , -1].values\n",
    "print(x)   #matrix of features\n",
    "print('\\n')\n",
    "print(y)   #dependent value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')   #missing_values: arguement which tells which is the missing \n",
    "#value, strategy: arguement which tells what should be filled in that missing value\n",
    "\n",
    "imputer.fit(x[: , 1:3])\n",
    "#fit method will connect this imputer to the matrix of features and in the arguements just put the numerical values column\n",
    "\n",
    "x[: , 1:3] = imputer.transform(x[: , 1:3])   #save the transformed column to those column\n",
    "#transform method will replace the missing value of each column with the mean of that column\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "#here the arguements will be transformers: (what kind of transfromation i.e.encoding, what kind of encoding, index of columns to encode) \n",
    "#remainder: put passthrough which tells you to keep columns that havent been encoded \n",
    "\n",
    "x = np.array(ct.fit_transform(x))   #return new matrix after fitting and transforming as we did above\n",
    "#compulsory to convert in numpy array as training expects you to have numpy array and fit_transform dont give you np array bydefault\n",
    "\n",
    "print(x)   #if your run this cell multiple times, every time a new encoded 0s & 1s pair will be used\n",
    "\n",
    "#in output france is encoded as 1.0 0.0 0.0 and spain is encoded as 0.0 0.0 1.0 and germany 0.0 1.0 0.0\n",
    "#where column order dont matter, apply ColumnTransformer with OneHotEncoder\n",
    "#where column order matter, apply LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#encoding the dependent variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)   # yes no column(last column)   #future machinery dont expect to have a numoy array of dependent varibale column\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "\n",
      "\n",
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "\n",
      "\n",
      "[0 1 0 0 1 1 0 1]\n",
      "\n",
      "\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "#test_size=0.2 means 20% observations in testset so the train_size=0.8 means 80% observations in trainset, divided by taking random values\n",
    "#random_state will give you the same training and test set even after running fro multiple time\n",
    "\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "print(x_test)\n",
    "print('\\n')\n",
    "print(y_train)\n",
    "print('\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Feature scaling is done after splitting dataset:\n",
    "* Featue scaling is done to make sure your features take value in the same scale, we do this to prevent one feature dominating other. Feature scaling computes the std-deviation and mean of the dataset for scaling the features of that dataset.\n",
    "* Test set is going to be the set on which your going to apply your machine learning model, so it should be new. \n",
    "* If we apply feature scaling before splitting then feature scaling will get the std-deviation and mean of all the data(original dataset). Getting information on test set which is going to have new observations that will be used for future production, will be like leaking the data.\n",
    "* We do feature scaling after splitting to prevent the data leakage on the test set that we are not supposed to have until training is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "\n",
      "\n",
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "#apply the standardisation(scaling technique) on x_tran y_train to scale the testset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train[: , 3:] = sc.fit_transform(x_train[: , 3:])   #apply the same scalar on testset that you applied on trainset so just apply transform method \n",
    "x_test[: , 3:] = sc.transform(x_test[: , 3:])\n",
    "#fit will just give you the mean and std-deviation of the values and transform will actually apply the standardisation formula\n",
    "\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Do we have to apply standardisation i.e. scaling to the dummy data i.e. 0.1.0.0.0.0 ? No , because we want to have all the features in the same scale. Standardisation transform your feature so they take values between -2 and +2, but our values of Countries are between +2 and -2. You can apply the standardisation to the dummy data above but you'll lose the whole the interpretation of the variables i.e. france like 1.0.0.0.0.0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
