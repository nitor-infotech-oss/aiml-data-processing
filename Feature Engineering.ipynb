{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technique to standardize the feature in a fix range regardless of the units of that particular feature. If it all its not perofrmed then ML models will assume the greater numbers as highest and smaller numbers as lowest regardless of the units of the features.\n",
    "* Two types: Standardization and Normalization.\n",
    "* Standardization: trasnform the data to have 0 mean and standard deviation as 1, it means that we control the spread of the data, we localize it around the center point.\n",
    "* Normalization: fixing the range of the data between two values, typically between 0-1 or 1 and -1.\n",
    "* Why we do it? \n",
    "  * ML models only sees the numbers and if in a column you have Rs.10 and the next corresponding column has 10gm, so the ML model will consider both these values as same regardless of their units, even if you have Rs.5 in one column and 10gm in another then 10gm will have superiority over Rs.5 as 10 is greater then 5 in terms of numbers.\n",
    "  * So we do feature scaling to make sure that there is no difference between the values and ranges of different features and that there has to be some standard range of all features and all the values should lie in that range so that it becomes easier to give them equal importance.\n",
    "* When to do it?\n",
    "  * When using ML algorithms that calculate **distance** of the data, for example while using unsupervised learning like K-means clustering to decide which of the features are closest together.\n",
    "  * KNN\n",
    "  * PCA\n",
    "* Algorithms that are rule based do not requires feature scaling fofr ex Random forest, Naive bayes.\n",
    "* We have Min, Max Scaler and Standard Scaler.\n",
    "  * Min, Max Scaler: Takes every value and subtracts with the minimum and then divides it with the difference between maximum and minimum, so its actually shrinking the data and makes sure that all the values lie between 0 and 1. We are minimum and maximum values so here the problem of outliers arises as outliers have extreme values so that maybe the disadvantage of the min,max scaler, that it is very sensitive towards outliers. \n",
    "  * Standard Scalers: Considers the distribution to be normal and transforms the data by changing the mean to zero and the standard deviation to one. Most popular scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "* Featire Engineering involves the study of features and understanding them and then engineering which involves *Combining, Removing adn Adding features*.\n",
    "* We have done removing by removing the features that were irrelevant bu using correlation matrix and combining the features that have same values or information by usng the categorical visualization and the adding part is new but have been done in Pandas while adding columns and features.\n",
    "* Engineering New Features: \n",
    "  * Combining: many features with same name can be combined as they can produce imbalance in the dataset .\n",
    "  * Removing: removing the features that won't impact the solution, for example ID or name in the real estate dataset won't be necessary to decide the pricing of the dataset, which we can remove by observing the correlation matrix.\n",
    "  * Adding: best way to add features or create new features is to use existing features using dummy variables. We can add a feature using *Categorical Column Grouping* and *Creating new features using old ones* and *Dummy Variables*\n",
    "  \n",
    "* Techniques for performing feature engineering:\n",
    "\n",
    "## Imputation\n",
    "* Missing values affect the performance of the ML models. Simple solution to the missing values is to drop the rows or the entire column.\n",
    "\n",
    "        value = 0.8  #fix threshold value\n",
    "        #save the columns which have missing value mean greater than fix threshold value and then drop them\n",
    "        data = data[data.columns[data.isnull().mean() > value]]\n",
    "        #save the rows which have missing value mean greater than fix threshold value and then drop them\n",
    "        data = data.loc[data.isnull().mean(axis=1) > value]\n",
    "        \n",
    "### Numerical Imputation\n",
    "* Filling the missing value is more preferable than dropping them. You can fill the missing value by '0' or by their 'median', as averages of the columns are sensitive to the outlier values.\n",
    "\n",
    "        #filling using 0\n",
    "        d = d.fillna(0)\n",
    "        #filling using median\n",
    "        d = d.fillna(d.median())\n",
    "      \n",
    "### Categorical Imputation\n",
    "* Replace the missing values with the maximum occurred value in a column.\n",
    "\n",
    "        d['column_name'].fillna(d['column_name'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers\n",
    "Detecting outliers with statistical methods. \n",
    "### Using Standard Deviation\n",
    "    upper_lim = d['column'].mean () + d['column'].std ()\n",
    "    lower_lim = d['column'].mean () - d['column'].std ()\n",
    "    d = d[(d['column'] < upper_lim) & (d['column'] > lower_lim)]\n",
    "### Using Percentiles\n",
    "Assume a certain percent of the value from the top or the bottom as an outlier.\n",
    "    \n",
    "    #dropping the outlier rows with percentile, top 10% means here the values that are out of the 90th percentile of data\n",
    "    upper_lim = d['column'].quantile(.90)\n",
    "    lower_lim = d['column'].quantile(.10)\n",
    "    d = d[(d['column'] < upper_lim) & (d['column'] > lower_lim)]\n",
    "    \n",
    "## Binning\n",
    "Applied on both categorical and numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   value   bin\n",
      "0      2   Low\n",
      "1     45   Mid\n",
      "2      7   Low\n",
      "3     86  High\n",
      "4     73  High\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#numerical binning\n",
    "data = pd.DataFrame({'value': (2,45,7,86,73)})\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Country      Continent\n",
      "0      Spain         Europe\n",
      "1      Chile  South America\n",
      "2  Australia          Other\n",
      "3      Italy         Europe\n",
      "4     Brazil  South America\n"
     ]
    }
   ],
   "source": [
    "#categorical binning\n",
    "\n",
    "dataa = pd.DataFrame({'Country': ('Spain','Chile','Australia','Italy','Brazil')})\n",
    "conditions = [\n",
    "    dataa['Country'].str.contains('Spain'),\n",
    "    dataa['Country'].str.contains('Italy'),\n",
    "    dataa['Country'].str.contains('Chile'),\n",
    "    dataa['Country'].str.contains('Brazil')]\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "dataa['Continent'] = np.select(conditions, choices, default='Other')\n",
    "print(dataa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "Most common encoding methods in machine learning. Undertand with following example.\n",
    "    \n",
    "    Fruit\tCategorical value of fruit\tPrice\n",
    "    apple\t 1\t                        5\n",
    "    mango\t 2\t                        10\n",
    "    apple\t 1\t                        15\n",
    "    orange\t3\t                        20\n",
    "After applying one hot encoder\n",
    "    \n",
    "    apple\tmango\torange\tprice\n",
    "    1\t     0\t      0\t   5\n",
    "    0\t     1\t      0\t   10\n",
    "    1\t     0\t      0\t   15\n",
    "    0\t     0\t      1\t   20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Zone  CreditScore\n",
      "0     1          619\n",
      "1     1          608\n",
      "2     3          502\n",
      "3     2          699\n",
      "4     1          850\n",
      "5     2          645\n",
      "6     2          822\n",
      "7     4          376\n",
      "8     2          501\n",
      "9     1          684\n",
      "[['1.0' '0.0' '0.0' '0.0' '619.0']\n",
      " ['1.0' '0.0' '0.0' '0.0' '608.0']\n",
      " ['0.0' '0.0' '1.0' '0.0' '502.0']\n",
      " ['0.0' '1.0' '0.0' '0.0' '699.0']\n",
      " ['1.0' '0.0' '0.0' '0.0' '850.0']\n",
      " ['0.0' '1.0' '0.0' '0.0' '645.0']\n",
      " ['0.0' '1.0' '0.0' '0.0' '822.0']\n",
      " ['0.0' '0.0' '0.0' '1.0' '376.0']\n",
      " ['0.0' '1.0' '0.0' '0.0' '501.0']\n",
      " ['1.0' '0.0' '0.0' '0.0' '684.0']]\n"
     ]
    }
   ],
   "source": [
    "#zone is a categorical value which needs to be one hot encoded\n",
    "dat = pd.DataFrame({'Zone':(1,1,3,2,1,2,2,4,2,1),\n",
    "                   'CreditScore': (619,608,502,699,850,645,822,376,501,684)})\n",
    "print(dat)\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer \n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])], remainder='passthrough') #[0], indicating the first column\n",
    "  \n",
    "dat = np.array(columnTransformer.fit_transform(dat), dtype = np.str) \n",
    "print(dat)   #firt four columns representing the zone(1,2,3,4) and last column as CreditScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The key point of group by operations is to decide the aggregation functions of the features.\n",
    "* For numerical features, average and sum functions are usually used, whereas for categorical features its more complicated.\n",
    "* Categorical Column Grouping\n",
    "  * The first way is to select the label with the highest frequency.\n",
    "  * Second way is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User      City  Visit Days\n",
      "0     1      Roma           1\n",
      "1     2    Madrid           2\n",
      "2     1    Madrid           1\n",
      "3     3  Isantbul           1\n",
      "4     2  Isantbul           4\n",
      "5     1  Isantbul           3\n",
      "6     1      Roma           3\n",
      "\n",
      "\n",
      "               Visit Days\n",
      "User City                \n",
      "1    Isantbul           3\n",
      "     Madrid             1\n",
      "     Roma               1\n",
      "2    Isantbul           4\n",
      "     Madrid             2\n",
      "3    Isantbul           1\n",
      "\n",
      "\n",
      "City  Isantbul  Madrid  Roma\n",
      "User                        \n",
      "1            3       1     4\n",
      "2            4       2     0\n",
      "3            1       0     0\n"
     ]
    }
   ],
   "source": [
    "da = pd.DataFrame({'User':(1,2,1,3,2,1,1,),\n",
    "                  'City':('Roma','Madrid','Madrid','Isantbul','Isantbul','Isantbul','Roma'),\n",
    "                  'Visit Days':(1,2,1,1,4,3,3)})\n",
    "print(da)\n",
    "\n",
    "#first way\n",
    "print('\\n')\n",
    "daa = da.groupby(['User','City']).agg(lambda x: x.value_counts().index[0])\n",
    "print(daa)\n",
    "\n",
    "#second way\n",
    "print('\\n')\n",
    "daaa = da.pivot_table(index='User', columns='City', values='Visit Days', aggfunc=np.sum, fill_value = 0)\n",
    "print(daaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Column Grouping\n",
    "* Numerical columns are grouped using sum and mean functions in most of the cases.\n",
    "\n",
    "        #sum_cols: List of columns to sum\n",
    "        #mean_cols: List of columns to average\n",
    "        grouped = data.groupby('column_to_group')\n",
    "\n",
    "        sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "        avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "        new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Split\n",
    "* Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Luther\n",
      "1    Charles\n",
      "2      Terry\n",
      "3    Kristen\n",
      "4     Thomas\n",
      "Name: name, dtype: object\n",
      "0    Gonzalez\n",
      "1       Young\n",
      "2      Lawson\n",
      "3       White\n",
      "4     Logsdon\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "datt = pd.DataFrame({'name':('Luther N. Gonzalez','Charles M. Young','Terry Lawson','Kristen White','Thomas Logsdon')})\n",
    "print(datt.name.str.split(\" \").map(lambda x: x[0]))   #Extracting last name\n",
    "print(datt.name.str.split(\" \").map(lambda x: x[-1]))   #Extracting first name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
